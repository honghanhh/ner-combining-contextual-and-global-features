{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dropout, Dense, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import pickle as pkl \n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from layers.graph import SpectralGraphConvolution\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_labels(labels):\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i][0] == \"I\":\n",
    "            if i == 0 or labels[i-1][2:] != labels[i][2:]:\n",
    "                labels[i] = \"B-{}\".format(labels[i][2:])\n",
    "    return labels\n",
    "\n",
    "\n",
    "def decode_labels(labels, idx2label):\n",
    "    labels = np.array(labels)\n",
    "    prediction_indices = labels.argmax(axis=1)\n",
    "    prediction_labels = [idx2label[i] for i in prediction_indices]\n",
    "    return prediction_labels\n",
    "\n",
    "\n",
    "def predict_labels(predictions, actuals, idx2label):\n",
    "    predictions_labels = []\n",
    "    actuals_labels = []\n",
    "    for i in range(len(predictions)):\n",
    "#     for i in range(predictions.shape[0]):\n",
    "        prediction = predictions[i]\n",
    "        actual = actuals[i]\n",
    "        prediction_labels = decode_labels(prediction, idx2label)\n",
    "        prediction_labels = fix_labels(prediction_labels)\n",
    "        actual_labels = decode_labels(actual, idx2label)\n",
    "        predictions_labels.append(prediction_labels)\n",
    "        actuals_labels.append(actual_labels)\n",
    "    return predictions_labels, actuals_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_pred):\n",
    "        ## calc metric\n",
    "    num_proposed = sum(1 for n in y_pred if n != 'O')\n",
    "    num_correct = 0\n",
    "    for i,j in zip(y_true,y_pred):\n",
    "        if i != 'O' and i == j:\n",
    "            num_correct +=1\n",
    "    num_gold = sum(1 for n in y_true if n != 'O')\n",
    "    print(\"num_proposed: \", num_proposed)\n",
    "    print(\"num_correct: \", num_correct)\n",
    "    print(\"num_gold: \", num_gold)\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "    final = \".P%.2f_R%.2f_F%.2f\" %(precision, recall, f1)\n",
    "    print(\"precision=%.4f\"%precision)\n",
    "    print(\"recall=%.4f\"%recall)\n",
    "    print(\"f1=%.4f\"%f1)\n",
    "    print(\"final \",final)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "        ## calc metric\n",
    "    y_pred, y_true = predict_labels(\n",
    "        y_pred, y_true, meta['idx2label'])\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = [x.split('-')[1] if '-' in x else x for x in y_pred[i]]\n",
    "    for i in range(len(y_true)):\n",
    "        y_true[i] = [x.split('-')[1] if '-' in x else x for x in y_true[i]]\n",
    "    \n",
    "    gt = []\n",
    "    pr = []\n",
    "    for i in range(len(y_pred)):\n",
    "        gt.extend(y_pred[i])\n",
    "    for i in range(len(y_true)):\n",
    "        pr.extend(y_true[i])\n",
    "        \n",
    "    num_proposed = sum(1 for n in pr if n != 'O')\n",
    "    num_correct = 0\n",
    "    for i,j in zip(gt,pr):\n",
    "        if i != 'O' and i == j:\n",
    "            num_correct +=1\n",
    "    num_gold = sum(1 for n in gt if n != 'O')\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'conll2003'\n",
    "EPOCHS = 4\n",
    "LR = 5e-5\n",
    "L2 = 0\n",
    "DO = 0.5\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading embedding matrix...\n",
      "Processing dataset...\n",
      "Number of nodes: 124\n",
      "Number of relations: 44\n",
      "Number of classes: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "\n",
    "A, X, Y, meta = pkl.load(open('pkl/' + DATASET + '.pkl', 'rb'))\n",
    "\n",
    "print(\"Loading embedding matrix...\")\n",
    "\n",
    "embedding_matrix = pkl.load(\n",
    "    open('pkl/' + DATASET + '.embedding_matrix.pkl', 'rb'))\n",
    "\n",
    "print(\"Processing dataset...\")\n",
    "\n",
    "val_y = load_output(A, X, Y, 'val')\n",
    "test_y = load_output(A, X, Y, 'test')\n",
    "\n",
    "num_nodes = A['train'][0][0].shape[0]\n",
    "num_relations = len(A['train'][0]) - 1\n",
    "num_labels = len(meta['label2idx'])\n",
    "\n",
    "print(\"Number of nodes: {}\".format(num_nodes))\n",
    "print(\"Number of relations: {}\".format(num_relations))\n",
    "print(\"Number of classes: {}\".format(num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model inputs\n",
    "X_in = Input(shape=(num_nodes, ))\n",
    "A_in = [Input(shape=(num_nodes, num_nodes)) for _ in range(num_relations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Define model\")\n",
    "# Define model architecture\n",
    "X_embedding = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[\n",
    "                        embedding_matrix], trainable=False)(X_in)\n",
    "H = SpectralGraphConvolution(256, activation='relu')([X_embedding] + A_in)\n",
    "H = Dropout(DO)(H)\n",
    "H = SpectralGraphConvolution(256, activation='relu')([H] + A_in)\n",
    "H = Dropout(DO)(H)\n",
    "output = Dense(num_labels, activation='softmax')(H)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in] + A_in, outputs=output)\n",
    "model.compile(metrics=['acc'],loss='categorical_crossentropy', optimizer=Adam(lr=LR))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [EarlyStopping(monitor='f1_metric', patience=2, verbose=0),\n",
    "#              ModelCheckpoint(filepath='model.{loss:.2f}.h5', monitor='f1_metric', save_best_only=True, verbose=0)\n",
    "#             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(\"=== EPOCH {} ===\".format(epoch + 1))\n",
    "\n",
    "    model.fit_generator(batch_generator(A, X, Y, 'train', batch_size=BATCH_SIZE),\n",
    "                        steps_per_epoch=len(A['train'])//BATCH_SIZE, verbose=1)\n",
    "\n",
    "\n",
    "    val_predictions = model.predict_generator(batch_generator(\n",
    "        A, X, Y, 'val', batch_size=BATCH_SIZE), steps=len(A['val'])//BATCH_SIZE, verbose=1)\n",
    "    val_predicted_labels, val_actual_labels = predict_labels(\n",
    "        val_predictions, val_y, meta['idx2label'])\n",
    "\n",
    "    for i in range(len(val_predicted_labels)):\n",
    "        val_predicted_labels[i] = [x.split('-')[1] if '-' in x else x for x in val_predicted_labels[i]]\n",
    "    for i in range(len(val_actual_labels)):\n",
    "        val_actual_labels[i] = [x.split('-')[1] if '-' in x else x for x in val_actual_labels[i]]\n",
    "    \n",
    "    gt = []\n",
    "    pr = []\n",
    "    for i in range(len(val_predicted_labels)):\n",
    "        gt.extend(val_predicted_labels[i])\n",
    "    for i in range(len(val_actual_labels)):\n",
    "        pr.extend(val_actual_labels[i])\n",
    "        \n",
    "    print(\"=== Validation Results ===\")\n",
    "    print(\"Weighted F1-score: \",f1_score(gt,pr, average = 'weighted'))\n",
    "    print(\"Classification report:\\n\", classification_report(gt,pr))\n",
    "    evaluate_metrics(gt, pr)\n",
    "\n",
    "    test_predictions = model.predict_generator(batch_generator(\n",
    "        A, X, Y, 'test', batch_size=BATCH_SIZE), steps=len(A['test']) // BATCH_SIZE, verbose=1)\n",
    "\n",
    "    test_predicted_labels, test_actual_labels = predict_labels(\n",
    "        test_predictions, test_y, meta['idx2label'])\n",
    "    for i in range(len(test_predicted_labels)):\n",
    "        test_predicted_labels[i] = [x.split('-')[1] if '-' in x else x for x in test_predicted_labels[i]]\n",
    "    for i in range(len(test_actual_labels)):\n",
    "        test_actual_labels[i] = [x.split('-')[1] if '-' in x else x for x in test_actual_labels[i]]\n",
    "\n",
    "    print(\"=== Test Results ===\")\n",
    "\n",
    "    gt = []\n",
    "    pr = []\n",
    "    for i in range(len(test_predicted_labels)):\n",
    "        gt.extend(test_predicted_labels[i])\n",
    "    for i in range(len(test_actual_labels)):\n",
    "        pr.extend(test_actual_labels[i])\n",
    "    print(\"Weighted F1-score: \",f1_score(gt,pr, average = 'weighted'))\n",
    "    print(\"Classification report:\\n\", classification_report(gt,pr))\n",
    "    evaluate_metrics(gt, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
