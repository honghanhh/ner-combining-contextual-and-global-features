{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "from os.path import dirname, join\n",
    "from data_load import NerDataset, pad, VOCAB, tag2idx, idx2tag\n",
    "from model import Net\n",
    "from conlleval import evaluate_conll_file\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='finetuning/4.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net(\n",
       "    (xlnet): XLNetModel(\n",
       "      (word_embedding): Embedding(32000, 768)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = torch.nn.DataParallel(*(list(model.module.children())[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "train_dataset = NerDataset('conll2003/train.txt')\n",
    "eval_dataset = NerDataset('conll2003/valid.txt')\n",
    "test_dataset = NerDataset('conll2003/test.txt')\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=16,\n",
    "                             num_workers=4,\n",
    "                             collate_fn=pad)\n",
    "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                            batch_size=16,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=pad)\n",
    "\n",
    "test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                            batch_size=16,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "11\n",
      "4\n",
      "4\n",
      "32\n",
      "33\n",
      "35\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "for sent in train_dataset.sents[:8]:\n",
    "    print(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "gcn_train = pkl.load(open('../conll_gcn/pkl/train_predictions.pkl', 'rb'))\n",
    "gcn_val = pkl.load(open('../conll_gcn/pkl/val_predictions.pkl', 'rb'))\n",
    "gcn_test = pkl.load(open('../conll_gcn/pkl/test_predictions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_train = gcn_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18447, 124, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 124, 256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_train[:8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, xln_model, gcn_pretrained, vocab_size, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        self.xln_model = xln_model\n",
    "        self.gcn_pretrained = gcn_pretrained\n",
    "\n",
    "        self.fc = nn.Linear(768 + 256, vocab_size)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, x, gcn_tensor):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "\n",
    "        Returns\n",
    "        '''\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        self.xln_model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = self.xln_model(x)\n",
    "            enc = encoded_layers[0]\n",
    "        gcn_tensor = torch.from_numpy(gcn_tensor).float()\n",
    "        gcn_tensor = gcn_tensor.to(self.device)\n",
    "        ensemble = torch.cat((enc, gcn_tensor), dim=2)\n",
    "        logits = self.fc(ensemble)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model = EnsembleModel(newmodel, gcn_train, vocab_size = len(VOCAB), device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (xln_model): DataParallel(\n",
       "    (module): XLNetModel(\n",
       "      (word_embedding): Embedding(32000, 768)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.97223162651062\n",
      "step: 10, loss: 2.2374839782714844\n",
      "step: 20, loss: 1.3089395761489868\n",
      "step: 30, loss: 2.462332010269165\n",
      "step: 40, loss: 1.5569565296173096\n",
      "step: 50, loss: 1.0468534231185913\n",
      "step: 60, loss: 1.0251495838165283\n",
      "step: 70, loss: 0.4931497275829315\n",
      "step: 80, loss: 0.468456894159317\n",
      "step: 90, loss: 0.41893771290779114\n",
      "step: 100, loss: 0.5957819819450378\n",
      "step: 110, loss: 0.3539648652076721\n",
      "step: 120, loss: 0.32535508275032043\n",
      "step: 130, loss: 0.5606241226196289\n",
      "step: 140, loss: 0.48682472109794617\n",
      "step: 150, loss: 0.2428196519613266\n",
      "step: 160, loss: 0.6382691264152527\n",
      "step: 170, loss: 0.16788998246192932\n",
      "step: 180, loss: 0.1333020180463791\n",
      "step: 190, loss: 0.1095779687166214\n",
      "step: 200, loss: 0.16327139735221863\n",
      "step: 210, loss: 0.18492990732192993\n",
      "step: 220, loss: 0.09266629070043564\n",
      "step: 230, loss: 0.21930654346942902\n",
      "step: 240, loss: 0.27662137150764465\n",
      "step: 250, loss: 0.21072933077812195\n",
      "step: 260, loss: 0.2820131182670593\n",
      "step: 270, loss: 0.16538234055042267\n",
      "step: 280, loss: 0.054466888308525085\n",
      "step: 290, loss: 0.08212988078594208\n",
      "step: 300, loss: 0.2093605399131775\n",
      "step: 310, loss: 0.3443554639816284\n",
      "step: 320, loss: 0.15633301436901093\n",
      "step: 330, loss: 0.03679946810007095\n",
      "step: 340, loss: 0.2523043155670166\n",
      "step: 350, loss: 0.06879260390996933\n",
      "step: 360, loss: 0.04368507117033005\n",
      "step: 370, loss: 0.0572052001953125\n",
      "step: 380, loss: 0.08665534108877182\n",
      "step: 390, loss: 0.0543195977807045\n",
      "step: 400, loss: 0.040195610374212265\n",
      "step: 410, loss: 0.02806692011654377\n",
      "step: 420, loss: 0.0823075994849205\n",
      "step: 430, loss: 0.06997431814670563\n",
      "step: 440, loss: 0.048450157046318054\n",
      "step: 450, loss: 0.07460862398147583\n",
      "step: 460, loss: 0.04086858406662941\n",
      "step: 470, loss: 0.04659675434231758\n",
      "step: 480, loss: 0.05531935393810272\n",
      "step: 490, loss: 0.02891543321311474\n",
      "step: 500, loss: 0.0428408607840538\n",
      "step: 510, loss: 0.03280651941895485\n",
      "step: 520, loss: 0.22572173178195953\n",
      "step: 530, loss: 0.0446123443543911\n",
      "step: 540, loss: 0.04718126356601715\n",
      "step: 550, loss: 0.031906966120004654\n",
      "step: 560, loss: 0.06485845893621445\n",
      "step: 570, loss: 0.033706553280353546\n",
      "step: 580, loss: 0.20716829597949982\n",
      "step: 590, loss: 0.027851467952132225\n",
      "step: 600, loss: 0.014536207541823387\n",
      "step: 610, loss: 0.019836025312542915\n",
      "step: 620, loss: 0.020672151818871498\n",
      "step: 630, loss: 0.02661404386162758\n",
      "step: 640, loss: 0.03756085783243179\n",
      "step: 650, loss: 0.03959416598081589\n",
      "step: 660, loss: 0.01228726003319025\n",
      "step: 670, loss: 0.045560237020254135\n",
      "step: 680, loss: 0.04208170250058174\n",
      "step: 690, loss: 0.03176761791110039\n",
      "step: 700, loss: 0.07412063330411911\n",
      "step: 710, loss: 0.06444848328828812\n",
      "step: 720, loss: 0.041908860206604004\n",
      "step: 730, loss: 0.12927860021591187\n",
      "step: 740, loss: 0.02488022670149803\n",
      "step: 750, loss: 0.03687894344329834\n",
      "step: 760, loss: 0.02466283179819584\n",
      "step: 770, loss: 0.01748286560177803\n",
      "step: 780, loss: 0.013300205580890179\n",
      "step: 790, loss: 0.06383398920297623\n",
      "step: 800, loss: 0.01081891916692257\n",
      "step: 810, loss: 0.058531828224658966\n",
      "step: 820, loss: 0.023436928167939186\n",
      "step: 830, loss: 0.020563671365380287\n",
      "step: 840, loss: 0.03772260993719101\n",
      "step: 850, loss: 0.01185667421668768\n",
      "step: 860, loss: 0.010507991537451744\n",
      "step: 870, loss: 0.04636164382100105\n",
      "step: 880, loss: 0.0345272533595562\n",
      "step: 890, loss: 0.0082519156858325\n",
      "step: 900, loss: 0.013720771297812462\n",
      "step: 910, loss: 0.014634509570896626\n",
      "step: 920, loss: 0.014334461651742458\n",
      "step: 930, loss: 0.01745169423520565\n",
      "step: 940, loss: 0.019948245957493782\n",
      "step: 950, loss: 0.01584586687386036\n",
      "step: 960, loss: 0.030272245407104492\n",
      "step: 970, loss: 0.014762136153876781\n",
      "step: 980, loss: 0.03237051144242287\n",
      "step: 990, loss: 0.007714400067925453\n",
      "step: 1000, loss: 0.01222908589988947\n",
      "step: 1010, loss: 0.006551453378051519\n",
      "step: 1020, loss: 0.014137727208435535\n",
      "step: 1030, loss: 0.005025372374802828\n",
      "step: 1040, loss: 0.020203806459903717\n",
      "step: 1050, loss: 0.020451577380299568\n",
      "step: 1060, loss: 0.019439322873950005\n",
      "step: 1070, loss: 0.0011636440176516771\n",
      "step: 1080, loss: 0.019070284441113472\n",
      "step: 1090, loss: 0.01218743808567524\n",
      "step: 1100, loss: 0.04377000406384468\n",
      "step: 1110, loss: 0.03765999153256416\n",
      "step: 1120, loss: 0.007102318573743105\n",
      "step: 1130, loss: 0.02372916042804718\n",
      "step: 1140, loss: 0.025664879009127617\n",
      "step: 1150, loss: 0.006162563804537058\n",
      "step: 0, loss: 0.005748840048909187\n",
      "step: 10, loss: 0.010095993056893349\n",
      "step: 20, loss: 0.007017350755631924\n",
      "step: 30, loss: 0.011221017688512802\n",
      "step: 40, loss: 0.07348895817995071\n",
      "step: 50, loss: 0.022141696885228157\n",
      "step: 60, loss: 0.022204609587788582\n",
      "step: 70, loss: 0.015124659053981304\n",
      "step: 80, loss: 0.010600370354950428\n",
      "step: 90, loss: 0.008191398344933987\n",
      "step: 100, loss: 0.02891763113439083\n",
      "step: 110, loss: 0.016522392630577087\n",
      "step: 120, loss: 0.015901027247309685\n",
      "step: 130, loss: 0.03521524369716644\n",
      "step: 140, loss: 0.01594436541199684\n",
      "step: 150, loss: 0.008489872328937054\n",
      "step: 160, loss: 0.010728470049798489\n",
      "step: 170, loss: 0.004628604277968407\n",
      "step: 180, loss: 0.006106415763497353\n",
      "step: 190, loss: 0.004035654477775097\n",
      "step: 200, loss: 0.009456342086195946\n",
      "step: 210, loss: 0.008177660405635834\n",
      "step: 220, loss: 0.006844427902251482\n",
      "step: 230, loss: 0.021749131381511688\n",
      "step: 240, loss: 0.02501864731311798\n",
      "step: 250, loss: 0.03973938897252083\n",
      "step: 260, loss: 0.05934218317270279\n",
      "step: 270, loss: 0.011910065077245235\n",
      "step: 280, loss: 0.022149089723825455\n",
      "step: 290, loss: 0.018219688907265663\n",
      "step: 300, loss: 0.017520369961857796\n",
      "step: 310, loss: 0.17613521218299866\n",
      "step: 320, loss: 0.019504424184560776\n",
      "step: 330, loss: 0.0019336428958922625\n",
      "step: 340, loss: 0.047384921461343765\n",
      "step: 350, loss: 0.00617624819278717\n",
      "step: 360, loss: 0.009775676764547825\n",
      "step: 370, loss: 0.02684669941663742\n",
      "step: 380, loss: 0.016587717458605766\n",
      "step: 390, loss: 0.025293821468949318\n",
      "step: 400, loss: 0.0046692476607859135\n",
      "step: 410, loss: 0.004602043889462948\n",
      "step: 420, loss: 0.03428097069263458\n",
      "step: 430, loss: 0.005815644282847643\n",
      "step: 440, loss: 0.008214452303946018\n",
      "step: 450, loss: 0.027543285861611366\n",
      "step: 460, loss: 0.009892088361084461\n",
      "step: 470, loss: 0.007332588080316782\n",
      "step: 480, loss: 0.007331376429647207\n",
      "step: 490, loss: 0.0038784435018897057\n",
      "step: 500, loss: 0.013291213661432266\n",
      "step: 510, loss: 0.018554653972387314\n",
      "step: 520, loss: 0.0751747116446495\n",
      "step: 530, loss: 0.007558252662420273\n",
      "step: 540, loss: 0.017415691167116165\n",
      "step: 550, loss: 0.005402091890573502\n",
      "step: 560, loss: 0.009795001707971096\n",
      "step: 570, loss: 0.011423228308558464\n",
      "step: 580, loss: 0.06770270317792892\n",
      "step: 590, loss: 0.005486402194947004\n",
      "step: 600, loss: 0.0025957045145332813\n",
      "step: 610, loss: 0.0028047487139701843\n",
      "step: 620, loss: 0.005272491369396448\n",
      "step: 630, loss: 0.010960227809846401\n",
      "step: 640, loss: 0.022798538208007812\n",
      "step: 650, loss: 0.01803002692759037\n",
      "step: 660, loss: 0.002047161804512143\n",
      "step: 670, loss: 0.011001688428223133\n",
      "step: 680, loss: 0.01767817884683609\n",
      "step: 690, loss: 0.006844333838671446\n",
      "step: 700, loss: 0.041796643286943436\n",
      "step: 710, loss: 0.03178012743592262\n",
      "step: 720, loss: 0.018198437988758087\n",
      "step: 730, loss: 0.11361196637153625\n",
      "step: 740, loss: 0.009701152332127094\n",
      "step: 750, loss: 0.03168375417590141\n",
      "step: 760, loss: 0.005620444659143686\n",
      "step: 770, loss: 0.007210325915366411\n",
      "step: 780, loss: 0.002778556663542986\n",
      "step: 790, loss: 0.031414855271577835\n",
      "step: 800, loss: 0.0022075336892157793\n",
      "step: 810, loss: 0.02295655384659767\n",
      "step: 820, loss: 0.005000134464353323\n",
      "step: 830, loss: 0.004647290334105492\n",
      "step: 840, loss: 0.029286736622452736\n",
      "step: 850, loss: 0.002512918785214424\n",
      "step: 860, loss: 0.0023512239567935467\n",
      "step: 870, loss: 0.0350007526576519\n",
      "step: 880, loss: 0.013013152405619621\n",
      "step: 890, loss: 0.0017552939243614674\n",
      "step: 900, loss: 0.008658803068101406\n",
      "step: 910, loss: 0.004203427117317915\n",
      "step: 920, loss: 0.011223558336496353\n",
      "step: 930, loss: 0.005183240864425898\n",
      "step: 940, loss: 0.005949557293206453\n",
      "step: 950, loss: 0.003920836374163628\n",
      "step: 960, loss: 0.02694128081202507\n",
      "step: 970, loss: 0.0035739170853048563\n",
      "step: 980, loss: 0.013945912942290306\n",
      "step: 990, loss: 0.0018519156146794558\n",
      "step: 1000, loss: 0.0033459309488534927\n",
      "step: 1010, loss: 0.0019457461312413216\n",
      "step: 1020, loss: 0.0039430297911167145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1030, loss: 0.0017528204480186105\n",
      "step: 1040, loss: 0.008346643298864365\n",
      "step: 1050, loss: 0.006653104908764362\n",
      "step: 1060, loss: 0.008463237434625626\n",
      "step: 1070, loss: 0.00028318105614744127\n",
      "step: 1080, loss: 0.010494867339730263\n",
      "step: 1090, loss: 0.008232113905251026\n",
      "step: 1100, loss: 0.017727728933095932\n",
      "step: 1110, loss: 0.02903505600988865\n",
      "step: 1120, loss: 0.0026537671219557524\n",
      "step: 1130, loss: 0.021265359595417976\n",
      "step: 1140, loss: 0.01818530075252056\n",
      "step: 1150, loss: 0.001997973769903183\n",
      "step: 0, loss: 0.001829238492064178\n",
      "step: 10, loss: 0.003328567836433649\n",
      "step: 20, loss: 0.002644812688231468\n",
      "step: 30, loss: 0.00316420616582036\n",
      "step: 40, loss: 0.03986021503806114\n",
      "step: 50, loss: 0.008281751535832882\n",
      "step: 60, loss: 0.011312411166727543\n",
      "step: 70, loss: 0.00783583428710699\n",
      "step: 80, loss: 0.004169153049588203\n",
      "step: 90, loss: 0.0034841829910874367\n",
      "step: 100, loss: 0.012219632044434547\n",
      "step: 110, loss: 0.008085445500910282\n",
      "step: 120, loss: 0.00838881079107523\n",
      "step: 130, loss: 0.03725800663232803\n",
      "step: 140, loss: 0.005704233422875404\n",
      "step: 150, loss: 0.003585237078368664\n",
      "step: 160, loss: 0.0034838628489524126\n",
      "step: 170, loss: 0.0017201885348185897\n",
      "step: 180, loss: 0.002530424389988184\n",
      "step: 190, loss: 0.0016967036062851548\n",
      "step: 200, loss: 0.004076316952705383\n",
      "step: 210, loss: 0.0027703656814992428\n",
      "step: 220, loss: 0.0031013134866952896\n",
      "step: 230, loss: 0.013618075288832188\n",
      "step: 240, loss: 0.005508017726242542\n",
      "step: 250, loss: 0.021092630922794342\n",
      "step: 260, loss: 0.035637713968753815\n",
      "step: 270, loss: 0.004867585375905037\n",
      "step: 280, loss: 0.026301966980099678\n",
      "step: 290, loss: 0.01583009772002697\n",
      "step: 300, loss: 0.0052496218122541904\n",
      "step: 310, loss: 0.1712936908006668\n",
      "step: 320, loss: 0.008771738968789577\n",
      "step: 330, loss: 0.0006132809212431312\n",
      "step: 340, loss: 0.021878842264413834\n",
      "step: 350, loss: 0.0021812906488776207\n",
      "step: 360, loss: 0.006246899254620075\n",
      "step: 370, loss: 0.02997957542538643\n",
      "step: 380, loss: 0.008048760704696178\n",
      "step: 390, loss: 0.02379954233765602\n",
      "step: 400, loss: 0.0019575871992856264\n",
      "step: 410, loss: 0.0023869527503848076\n",
      "step: 420, loss: 0.031123772263526917\n",
      "step: 430, loss: 0.0021004010923206806\n",
      "step: 440, loss: 0.003991366364061832\n",
      "step: 450, loss: 0.021711289882659912\n",
      "step: 460, loss: 0.005417012143880129\n",
      "step: 470, loss: 0.003439000342041254\n",
      "step: 480, loss: 0.003330895444378257\n",
      "step: 490, loss: 0.0015751284081488848\n",
      "step: 500, loss: 0.012207712978124619\n",
      "step: 510, loss: 0.02057461254298687\n",
      "step: 520, loss: 0.04118657484650612\n",
      "step: 530, loss: 0.0033517898991703987\n",
      "step: 540, loss: 0.01411871425807476\n",
      "step: 550, loss: 0.002187797101214528\n",
      "step: 560, loss: 0.0035597828682512045\n",
      "step: 570, loss: 0.008820146322250366\n",
      "step: 580, loss: 0.01849360764026642\n",
      "step: 590, loss: 0.0020927346777170897\n",
      "step: 600, loss: 0.0011752662248909473\n",
      "step: 610, loss: 0.0012124681379646063\n",
      "step: 620, loss: 0.0028090544510632753\n",
      "step: 630, loss: 0.009777292609214783\n",
      "step: 640, loss: 0.02164405584335327\n",
      "step: 650, loss: 0.013595339842140675\n",
      "step: 660, loss: 0.0008500018739141524\n",
      "step: 670, loss: 0.005001499317586422\n",
      "step: 680, loss: 0.015200826339423656\n",
      "step: 690, loss: 0.002916767029091716\n",
      "step: 700, loss: 0.032378699630498886\n",
      "step: 710, loss: 0.021317927166819572\n",
      "step: 720, loss: 0.011011263355612755\n",
      "step: 730, loss: 0.10614348202943802\n",
      "step: 740, loss: 0.006516453344374895\n",
      "step: 750, loss: 0.03321490436792374\n",
      "step: 760, loss: 0.0027053288649767637\n",
      "step: 770, loss: 0.006485748570412397\n",
      "step: 780, loss: 0.0013992066960781813\n",
      "step: 790, loss: 0.02062750793993473\n",
      "step: 800, loss: 0.0011705452343448997\n",
      "step: 810, loss: 0.014097672887146473\n",
      "step: 820, loss: 0.0022549498826265335\n",
      "step: 830, loss: 0.0022337965201586485\n",
      "step: 840, loss: 0.02677498757839203\n",
      "step: 850, loss: 0.0011513808276504278\n",
      "step: 860, loss: 0.001050884835422039\n",
      "step: 870, loss: 0.034666746854782104\n",
      "step: 880, loss: 0.008360750041902065\n",
      "step: 890, loss: 0.0008373361197300255\n",
      "step: 900, loss: 0.007854649797081947\n",
      "step: 910, loss: 0.0020803820807486773\n",
      "step: 920, loss: 0.01192640233784914\n",
      "step: 930, loss: 0.00281973066739738\n",
      "step: 940, loss: 0.003259531455114484\n",
      "step: 950, loss: 0.0019632822368294\n",
      "step: 960, loss: 0.02662588469684124\n",
      "step: 970, loss: 0.0016302630538120866\n",
      "step: 980, loss: 0.00917157530784607\n",
      "step: 990, loss: 0.0008583668386563659\n",
      "step: 1000, loss: 0.0016406583599746227\n",
      "step: 1010, loss: 0.0011047213338315487\n",
      "step: 1020, loss: 0.00186753342859447\n",
      "step: 1030, loss: 0.0009893145179376006\n",
      "step: 1040, loss: 0.005466714967042208\n",
      "step: 1050, loss: 0.003664771094918251\n",
      "step: 1060, loss: 0.004997069016098976\n",
      "step: 1070, loss: 0.00012381051783449948\n",
      "step: 1080, loss: 0.008241645060479641\n",
      "step: 1090, loss: 0.007186755537986755\n",
      "step: 1100, loss: 0.010845432989299297\n",
      "step: 1110, loss: 0.028150923550128937\n",
      "step: 1120, loss: 0.0016450961120426655\n",
      "step: 1130, loss: 0.022711822763085365\n",
      "step: 1140, loss: 0.015312443487346172\n",
      "step: 1150, loss: 0.001099830842576921\n",
      "step: 0, loss: 0.0010564707918092608\n",
      "step: 10, loss: 0.0019282519351691008\n",
      "step: 20, loss: 0.001560127129778266\n",
      "step: 30, loss: 0.001501272083260119\n",
      "step: 40, loss: 0.026018360629677773\n",
      "step: 50, loss: 0.004103131592273712\n",
      "step: 60, loss: 0.007613515481352806\n",
      "step: 70, loss: 0.005904394667595625\n",
      "step: 80, loss: 0.002534158295020461\n",
      "step: 90, loss: 0.002307900693267584\n",
      "step: 100, loss: 0.007352293934673071\n",
      "step: 110, loss: 0.005541037768125534\n",
      "step: 120, loss: 0.00657753786072135\n",
      "step: 130, loss: 0.04016334190964699\n",
      "step: 140, loss: 0.003522175829857588\n",
      "step: 150, loss: 0.002018954837694764\n",
      "step: 160, loss: 0.0017780876951292157\n",
      "step: 170, loss: 0.0011020887177437544\n",
      "step: 180, loss: 0.0016434071585536003\n",
      "step: 190, loss: 0.001128159579820931\n",
      "step: 200, loss: 0.0026679239235818386\n",
      "step: 210, loss: 0.0014478923985734582\n",
      "step: 220, loss: 0.002004374284297228\n",
      "step: 230, loss: 0.009783362038433552\n",
      "step: 240, loss: 0.0024958974681794643\n",
      "step: 250, loss: 0.014199917204678059\n",
      "step: 260, loss: 0.024338284507393837\n",
      "step: 270, loss: 0.0031017304863780737\n",
      "step: 280, loss: 0.030081629753112793\n",
      "step: 290, loss: 0.016526123508810997\n",
      "step: 300, loss: 0.002389863831922412\n",
      "step: 310, loss: 0.1696489006280899\n",
      "step: 320, loss: 0.005066599231213331\n",
      "step: 330, loss: 0.0003112500417046249\n",
      "step: 340, loss: 0.013488230295479298\n",
      "step: 350, loss: 0.0011276593431830406\n",
      "step: 360, loss: 0.004892806056886911\n",
      "step: 370, loss: 0.033747341483831406\n",
      "step: 380, loss: 0.005133613012731075\n",
      "step: 390, loss: 0.02483586221933365\n",
      "step: 400, loss: 0.0012431573122739792\n",
      "step: 410, loss: 0.0015873225638642907\n",
      "step: 420, loss: 0.030441448092460632\n",
      "step: 430, loss: 0.0010916369501501322\n",
      "step: 440, loss: 0.0026360393967479467\n",
      "step: 450, loss: 0.01975802518427372\n",
      "step: 460, loss: 0.0038436069153249264\n",
      "step: 470, loss: 0.0023531231563538313\n",
      "step: 480, loss: 0.002152351662516594\n",
      "step: 490, loss: 0.0009044427424669266\n",
      "step: 500, loss: 0.012547561898827553\n",
      "step: 510, loss: 0.022716498002409935\n",
      "step: 520, loss: 0.026648331433534622\n",
      "step: 530, loss: 0.0019988161511719227\n",
      "step: 540, loss: 0.013462541624903679\n",
      "step: 550, loss: 0.0012463948223739862\n",
      "step: 560, loss: 0.0017750586848706007\n",
      "step: 570, loss: 0.008370285853743553\n",
      "step: 580, loss: 0.014841742813587189\n",
      "step: 590, loss: 0.0012012887746095657\n",
      "step: 600, loss: 0.0007704455638304353\n",
      "step: 610, loss: 0.0007914796588011086\n",
      "step: 620, loss: 0.0018121420871466398\n",
      "step: 630, loss: 0.009764782153069973\n",
      "step: 640, loss: 0.021957388147711754\n",
      "step: 650, loss: 0.012825535610318184\n",
      "step: 660, loss: 0.0004923680680803955\n",
      "step: 670, loss: 0.003098771907389164\n",
      "step: 680, loss: 0.01517552975565195\n",
      "step: 690, loss: 0.0019211017061024904\n",
      "step: 700, loss: 0.02834291011095047\n",
      "step: 710, loss: 0.01581716537475586\n",
      "step: 720, loss: 0.007505886722356081\n",
      "step: 730, loss: 0.10003861784934998\n",
      "step: 740, loss: 0.005288916174322367\n",
      "step: 750, loss: 0.034680675715208054\n",
      "step: 760, loss: 0.0017543720314279199\n",
      "step: 770, loss: 0.006578775122761726\n",
      "step: 780, loss: 0.0009914328111335635\n",
      "step: 790, loss: 0.01504412479698658\n",
      "step: 800, loss: 0.0008981816354207695\n",
      "step: 810, loss: 0.01024557650089264\n",
      "step: 820, loss: 0.0013640327379107475\n",
      "step: 830, loss: 0.0014571452047675848\n",
      "step: 840, loss: 0.02524169161915779\n",
      "step: 850, loss: 0.0007329694926738739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 860, loss: 0.0006469834479503334\n",
      "step: 870, loss: 0.03518490493297577\n",
      "step: 880, loss: 0.0064475093968212605\n",
      "step: 890, loss: 0.0005716904997825623\n",
      "step: 900, loss: 0.007693753577768803\n",
      "step: 910, loss: 0.0013304564636200666\n",
      "step: 920, loss: 0.012865865603089333\n",
      "step: 930, loss: 0.0019774052780121565\n",
      "step: 940, loss: 0.0023338410537689924\n",
      "step: 950, loss: 0.001336258021183312\n",
      "step: 960, loss: 0.025589562952518463\n",
      "step: 970, loss: 0.0009779100073501468\n",
      "step: 980, loss: 0.007097853813320398\n",
      "step: 990, loss: 0.0005402073729783297\n",
      "step: 1000, loss: 0.0010787791106849909\n",
      "step: 1010, loss: 0.0008144948515109718\n",
      "step: 1020, loss: 0.001153280376456678\n",
      "step: 1030, loss: 0.0006649584393016994\n",
      "step: 1040, loss: 0.004265468567609787\n",
      "step: 1050, loss: 0.002525780815631151\n",
      "step: 1060, loss: 0.0034459487069398165\n",
      "step: 1070, loss: 7.096657645888627e-05\n",
      "step: 1080, loss: 0.007209317293018103\n",
      "step: 1090, loss: 0.006534217856824398\n",
      "step: 1100, loss: 0.007921242155134678\n",
      "step: 1110, loss: 0.028232483193278313\n",
      "step: 1120, loss: 0.001310294377617538\n",
      "step: 1130, loss: 0.024119701236486435\n",
      "step: 1140, loss: 0.013634798116981983\n",
      "step: 1150, loss: 0.0007630077889189124\n"
     ]
    }
   ],
   "source": [
    "for ep in range(4):\n",
    "    ensemble_model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        words, x, is_heads, _, y, seqlens = batch\n",
    "        \n",
    "        max_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        idx = i * batch_size\n",
    "        \n",
    "        if idx+batch_size > gcn_train.shape[0]:\n",
    "            break\n",
    "        gcn_tensor = gcn_train[idx:idx+batch_size]\n",
    "        padded = np.zeros((batch_size, max_len, 256))\n",
    "\n",
    "        for ix in range(batch_size):\n",
    "            is_heads[ix][0] = is_heads[ix][-1] = 0\n",
    "            num_word = sum(is_heads[ix])\n",
    "            indexs = [head == 1 for head in is_heads[ix]] + [False] * (max_len - len(is_heads[ix]))\n",
    "            padded[ix][indexs]=gcn_tensor[ix][:num_word]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = ensemble_model(x, padded)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        y = y.to(device).view(-1)\n",
    "        \n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:  # monitoring\n",
    "            print(f\"step: {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> DHAKA 1996-08-31 <sep>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 5, 5, 5, 1, 0, 5, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 6, 1, 1, 1, 0, 1,\n",
       "         1, 1, 0, 0],\n",
       "        [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 4, 0, 4, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 1, 4, 0, 0, 4, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, gcn, f):\n",
    "    ensemble_model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "            max_len = x.shape[1]\n",
    "            batch_size = x.shape[0]\n",
    "            idx = i * batch_size\n",
    "\n",
    "            if idx+batch_size > gcn.shape[0]:\n",
    "                break\n",
    "            gcn_tensor = gcn[idx:idx+batch_size]\n",
    "            padded = np.zeros((batch_size, max_len, 256))\n",
    "\n",
    "            for ix in range(batch_size):\n",
    "                is_heads[ix][0] = is_heads[ix][-1] = 0\n",
    "                num_word = sum(is_heads[ix])\n",
    "                indexs = [head == 1 for head in is_heads[ix]] + [False] * (max_len - len(is_heads[ix]))\n",
    "                padded[ix][indexs]=gcn_tensor[ix][:num_word]\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "            logits = ensemble_model(x, padded)\n",
    "            y_hat = logits.argmax(-1)\n",
    "#             logits = logits.view(-1, logits.shape[-1])\n",
    "#             y = y.to(device).view(-1)\n",
    "\n",
    "#             _, _,y_hat = ensemble_model(x, padded)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist()) \n",
    "\n",
    "                 \n",
    "    # gets results and save\n",
    "    with open(\"temp\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            is_heads[0] = is_heads[-1] = 1\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "#             print(is_heads)\n",
    "#             print(preds)\n",
    "#             print(words.split())\n",
    "#             print(tags.split())\n",
    "            assert len(preds) == len(words.split()) == len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(f\"{w} {t} {p}\\n\")\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    with open(\"temp\") as fout:\n",
    "        evaluate_conll_file(fout)\n",
    "\n",
    "    # calc metric\n",
    "    y_true = np.array([tag2idx[line.split()[1]] for line in open(\n",
    "        \"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred = np.array([tag2idx[line.split()[2]] for line in open(\n",
    "        \"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    num_proposed = len(y_pred[y_pred > 1])\n",
    "    num_correct = (np.logical_and(y_true == y_pred,\n",
    "                                  y_true > 1)).astype(np.int).sum()\n",
    "    num_gold = len(y_true[y_true > 1])\n",
    "\n",
    "    print(f\"num_proposed:{num_proposed}\")\n",
    "    print(f\"num_correct:{num_correct}\")\n",
    "    print(f\"num_gold:{num_gold}\")\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall == 0:\n",
    "            f1 = 1.0\n",
    "        else:\n",
    "            f1 = 0\n",
    "\n",
    "    final = f + \".P%.2f_R%.2f_F%.2f\" % (precision, recall, f1)\n",
    "    with open(final, 'w') as fout:\n",
    "        result = open(\"temp\", \"r\").read()\n",
    "        fout.write(f\"{result}\\n\")\n",
    "\n",
    "        fout.write(f\"precision={precision}\\n\")\n",
    "        fout.write(f\"recall={recall}\\n\")\n",
    "        fout.write(f\"f1={f1}\\n\")\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    print(\"precision=%.4f\" % precision)\n",
    "    print(\"recall=%.4f\" % recall)\n",
    "    print(\"f1=%.4f\" % f1)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'checkpoints/02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========eval=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5975 phrases; correct: 5859.\n",
      "accuracy:  99.09%; (non-O)\n",
      "accuracy:  99.78%; precision:  98.06%; recall:  98.60%; FB1:  98.33\n",
      "              LOC: precision:  99.08%; recall:  99.62%; FB1:  99.35  1847\n",
      "             MISC: precision:  95.35%; recall:  95.55%; FB1:  95.45  924\n",
      "              ORG: precision:  97.48%; recall:  98.14%; FB1:  97.81  1350\n",
      "              PER: precision:  98.81%; recall:  99.46%; FB1:  99.13  1854\n",
      "num_proposed:8618\n",
      "num_correct:8525\n",
      "num_gold:8603\n",
      "precision=0.9892\n",
      "recall=0.9909\n",
      "f1=0.9901\n",
      "weights were saved to checkpoints/02/10.pt\n",
      "=========test0=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5787 phrases; correct: 5131.\n",
      "accuracy:  92.67%; (non-O)\n",
      "accuracy:  98.15%; precision:  88.66%; recall:  90.85%; FB1:  89.74\n",
      "              LOC: precision:  91.61%; recall:  92.33%; FB1:  91.97  1681\n",
      "             MISC: precision:  77.73%; recall:  81.05%; FB1:  79.36  732\n",
      "              ORG: precision:  85.89%; recall:  88.32%; FB1:  87.09  1708\n",
      "              PER: precision:  93.34%; recall:  96.17%; FB1:  94.73  1666\n",
      "num_proposed:8330\n",
      "num_correct:7517\n",
      "num_gold:8112\n",
      "precision=0.9024\n",
      "recall=0.9267\n",
      "f1=0.9144\n"
     ]
    }
   ],
   "source": [
    "print(f\"=========eval=========\")\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "fname = os.path.join(logdir, \"10\")\n",
    "precision, recall, f1 = eval(ensemble_model, eval_iter, gcn_val, fname)\n",
    "\n",
    "torch.save(ensemble_model, f\"{fname}.pt\")\n",
    "print(f\"weights were saved to {fname}.pt\")\n",
    "print(f\"=========test0=========\")\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "fname = os.path.join(logdir, \"10\" +'_test_')\n",
    "precision, recall, f1 = eval(ensemble_model, test_iter, gcn_test, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
