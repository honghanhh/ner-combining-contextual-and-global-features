{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "from os.path import dirname, join\n",
    "from data_load import NerDataset, pad, VOCAB, tag2idx, idx2tag\n",
    "from model import Net\n",
    "from conlleval import evaluate_conll_file\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "manualSeed = 1\n",
    "\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# if you are suing GPU\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='finetuning/4.pt'\n",
    "logdir = 'checkpoints/02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net(\n",
       "    (xlnet): XLNetModel(\n",
       "      (word_embedding): Embedding(32000, 768)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = torch.nn.DataParallel(*(list(model.module.children())[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "train_dataset = NerDataset('conll2003/train.txt')\n",
    "eval_dataset = NerDataset('conll2003/valid.txt')\n",
    "test_dataset = NerDataset('conll2003/test.txt')\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=16,\n",
    "                             num_workers=4,\n",
    "                             collate_fn=pad)\n",
    "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                            batch_size=16,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=pad)\n",
    "\n",
    "test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                            batch_size=16,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in train_dataset.sents[:8]:\n",
    "#     print(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "gcn_train = pkl.load(open('../conll_gcn/pkl/train_predictions.pkl', 'rb'))\n",
    "gcn_val = pkl.load(open('../conll_gcn/pkl/val_predictions.pkl', 'rb'))\n",
    "gcn_test = pkl.load(open('../conll_gcn/pkl/test_predictions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_train = gcn_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_train[:8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, xln_model, gcn_pretrained, vocab_size, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        self.xln_model = xln_model\n",
    "        self.gcn_pretrained = gcn_pretrained\n",
    "\n",
    "        self.fc = nn.Linear(768 + 256, vocab_size)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, x, gcn_tensor):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "\n",
    "        Returns\n",
    "        '''\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        self.xln_model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = self.xln_model(x)\n",
    "            enc = encoded_layers[0]\n",
    "        gcn_tensor = torch.from_numpy(gcn_tensor).float()\n",
    "        gcn_tensor = gcn_tensor.to(self.device)\n",
    "        ensemble = torch.cat((enc, gcn_tensor), dim=2)\n",
    "        logits = self.fc(ensemble)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model = EnsembleModel(newmodel, gcn_train, vocab_size = len(VOCAB), device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (xln_model): DataParallel(\n",
       "    (module): XLNetModel(\n",
       "      (word_embedding): Embedding(32000, 768)\n",
       "      (layer): ModuleList(\n",
       "        (0): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): XLNetLayer(\n",
       "          (rel_attn): XLNetRelativeAttention(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ff): XLNetFeedForward(\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[0:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, gcn, f):\n",
    "    ensemble_model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "            max_len = x.shape[1]\n",
    "            batch_size = x.shape[0]\n",
    "            idx = i * batch_size\n",
    "\n",
    "            if idx+batch_size > gcn.shape[0]:\n",
    "                break\n",
    "            gcn_tensor = gcn[idx:idx+batch_size]\n",
    "            padded = np.zeros((batch_size, max_len, 256))\n",
    "\n",
    "            for ix in range(batch_size):\n",
    "                is_heads[ix][0] = is_heads[ix][-1] = 0\n",
    "                num_word = sum(is_heads[ix])\n",
    "                indexs = [head == 1 for head in is_heads[ix]] + [False] * (max_len - len(is_heads[ix]))\n",
    "                padded[ix][indexs]=gcn_tensor[ix][:num_word]\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "            logits = ensemble_model(x, padded)\n",
    "            y_hat = logits.argmax(-1)\n",
    "#             logits = logits.view(-1, logits.shape[-1])\n",
    "#             y = y.to(device).view(-1)\n",
    "\n",
    "#             _, _,y_hat = ensemble_model(x, padded)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist()) \n",
    "\n",
    "                 \n",
    "    # gets results and save\n",
    "    with open(\"temp\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            is_heads[0] = is_heads[-1] = 1\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "#             print(is_heads)\n",
    "#             print(preds)\n",
    "#             print(words.split())\n",
    "#             print(tags.split())\n",
    "            assert len(preds) == len(words.split()) == len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(f\"{w} {t} {p}\\n\")\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    with open(\"temp\") as fout:\n",
    "        evaluate_conll_file(fout)\n",
    "\n",
    "    # calc metric\n",
    "    y_true = np.array([tag2idx[line.split()[1]] for line in open(\n",
    "        \"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred = np.array([tag2idx[line.split()[2]] for line in open(\n",
    "        \"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    num_proposed = len(y_pred[y_pred > 1])\n",
    "    num_correct = (np.logical_and(y_true == y_pred,\n",
    "                                  y_true > 1)).astype(np.int).sum()\n",
    "    num_gold = len(y_true[y_true > 1])\n",
    "\n",
    "    print(f\"num_proposed:{num_proposed}\")\n",
    "    print(f\"num_correct:{num_correct}\")\n",
    "    print(f\"num_gold:{num_gold}\")\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall == 0:\n",
    "            f1 = 1.0\n",
    "        else:\n",
    "            f1 = 0\n",
    "\n",
    "    final = f + \".P%.2f_R%.2f_F%.2f\" % (precision, recall, f1)\n",
    "    with open(final, 'w') as fout:\n",
    "        result = open(\"temp\", \"r\").read()\n",
    "        fout.write(f\"{result}\\n\")\n",
    "\n",
    "        fout.write(f\"precision={precision}\\n\")\n",
    "        fout.write(f\"recall={recall}\\n\")\n",
    "        fout.write(f\"f1={f1}\\n\")\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    print(\"precision=%.4f\" % precision)\n",
    "    print(\"recall=%.4f\" % recall)\n",
    "    print(\"f1=%.4f\" % f1)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========eval at epoch=0=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5960 phrases; correct: 5860.\n",
      "accuracy:  99.13%; (non-O)\n",
      "accuracy:  99.82%; precision:  98.32%; recall:  98.62%; FB1:  98.47\n",
      "              LOC: precision:  98.81%; recall:  99.62%; FB1:  99.21  1852\n",
      "             MISC: precision:  96.29%; recall:  95.77%; FB1:  96.03  917\n",
      "              ORG: precision:  97.99%; recall:  98.28%; FB1:  98.14  1345\n",
      "              PER: precision:  99.08%; recall:  99.29%; FB1:  99.19  1846\n",
      "num_proposed:8594\n",
      "num_correct:8528\n",
      "num_gold:8603\n",
      "precision=0.9923\n",
      "recall=0.9913\n",
      "f1=0.9918\n",
      "weights were saved to checkpoints/02/0.pt\n",
      "=========test at epoch=0=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5688 phrases; correct: 5107.\n",
      "accuracy:  92.25%; (non-O)\n",
      "accuracy:  98.32%; precision:  89.79%; recall:  90.42%; FB1:  90.10\n",
      "              LOC: precision:  90.69%; recall:  92.81%; FB1:  91.73  1707\n",
      "             MISC: precision:  79.20%; recall:  78.63%; FB1:  78.91  697\n",
      "              ORG: precision:  88.30%; recall:  87.24%; FB1:  87.76  1641\n",
      "              PER: precision:  94.83%; recall:  96.35%; FB1:  95.58  1643\n",
      "num_proposed:8120\n",
      "num_correct:7483\n",
      "num_gold:8112\n",
      "precision=0.9216\n",
      "recall=0.9225\n",
      "f1=0.9220\n",
      "=========eval at epoch=1=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5956 phrases; correct: 5869.\n",
      "accuracy:  99.26%; (non-O)\n",
      "accuracy:  99.83%; precision:  98.54%; recall:  98.77%; FB1:  98.66\n",
      "              LOC: precision:  99.19%; recall:  99.62%; FB1:  99.40  1845\n",
      "             MISC: precision:  96.62%; recall:  96.10%; FB1:  96.36  917\n",
      "              ORG: precision:  98.00%; recall:  98.58%; FB1:  98.29  1349\n",
      "              PER: precision:  99.24%; recall:  99.40%; FB1:  99.32  1845\n",
      "num_proposed:8599\n",
      "num_correct:8539\n",
      "num_gold:8603\n",
      "precision=0.9930\n",
      "recall=0.9926\n",
      "f1=0.9928\n",
      "weights were saved to checkpoints/02/1.pt\n",
      "=========test at epoch=1=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5692 phrases; correct: 5117.\n",
      "accuracy:  92.42%; (non-O)\n",
      "accuracy:  98.33%; precision:  89.90%; recall:  90.60%; FB1:  90.25\n",
      "              LOC: precision:  91.35%; recall:  92.45%; FB1:  91.90  1688\n",
      "             MISC: precision:  79.39%; recall:  78.49%; FB1:  78.94  694\n",
      "              ORG: precision:  88.18%; recall:  88.02%; FB1:  88.10  1658\n",
      "              PER: precision:  94.55%; recall:  96.60%; FB1:  95.56  1652\n",
      "num_proposed:8141\n",
      "num_correct:7497\n",
      "num_gold:8112\n",
      "precision=0.9209\n",
      "recall=0.9242\n",
      "f1=0.9225\n",
      "=========eval at epoch=2=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5951 phrases; correct: 5875.\n",
      "accuracy:  99.31%; (non-O)\n",
      "accuracy:  99.85%; precision:  98.72%; recall:  98.87%; FB1:  98.80\n",
      "              LOC: precision:  99.40%; recall:  99.62%; FB1:  99.51  1841\n",
      "             MISC: precision:  96.95%; recall:  96.42%; FB1:  96.68  917\n",
      "              ORG: precision:  98.00%; recall:  98.73%; FB1:  98.37  1351\n",
      "              PER: precision:  99.46%; recall:  99.46%; FB1:  99.46  1842\n",
      "num_proposed:8598\n",
      "num_correct:8544\n",
      "num_gold:8603\n",
      "precision=0.9937\n",
      "recall=0.9931\n",
      "f1=0.9934\n",
      "weights were saved to checkpoints/02/2.pt\n",
      "=========test at epoch=2=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5693 phrases; correct: 5128.\n",
      "accuracy:  92.60%; (non-O)\n",
      "accuracy:  98.36%; precision:  90.08%; recall:  90.79%; FB1:  90.43\n",
      "              LOC: precision:  91.85%; recall:  92.63%; FB1:  92.24  1682\n",
      "             MISC: precision:  79.48%; recall:  78.92%; FB1:  79.20  697\n",
      "              ORG: precision:  88.33%; recall:  88.44%; FB1:  88.39  1663\n",
      "              PER: precision:  94.49%; recall:  96.47%; FB1:  95.47  1651\n",
      "num_proposed:8145\n",
      "num_correct:7512\n",
      "num_gold:8112\n",
      "precision=0.9223\n",
      "recall=0.9260\n",
      "f1=0.9242\n",
      "=========eval at epoch=3=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5951 phrases; correct: 5880.\n",
      "accuracy:  99.36%; (non-O)\n",
      "accuracy:  99.85%; precision:  98.81%; recall:  98.96%; FB1:  98.88\n",
      "              LOC: precision:  99.56%; recall:  99.67%; FB1:  99.62  1839\n",
      "             MISC: precision:  97.28%; recall:  96.85%; FB1:  97.07  918\n",
      "              ORG: precision:  97.86%; recall:  98.66%; FB1:  98.25  1352\n",
      "              PER: precision:  99.51%; recall:  99.51%; FB1:  99.51  1842\n",
      "num_proposed:8599\n",
      "num_correct:8548\n",
      "num_gold:8603\n",
      "precision=0.9941\n",
      "recall=0.9936\n",
      "f1=0.9938\n",
      "weights were saved to checkpoints/02/3.pt\n",
      "=========test at epoch=3=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5694 phrases; correct: 5135.\n",
      "accuracy:  92.73%; (non-O)\n",
      "accuracy:  98.38%; precision:  90.18%; recall:  90.92%; FB1:  90.55\n",
      "              LOC: precision:  92.06%; recall:  92.51%; FB1:  92.28  1676\n",
      "             MISC: precision:  79.86%; recall:  79.63%; FB1:  79.74  700\n",
      "              ORG: precision:  88.30%; recall:  88.62%; FB1:  88.46  1667\n",
      "              PER: precision:  94.55%; recall:  96.54%; FB1:  95.53  1651\n",
      "num_proposed:8156\n",
      "num_correct:7522\n",
      "num_gold:8112\n",
      "precision=0.9223\n",
      "recall=0.9273\n",
      "f1=0.9248\n",
      "=========eval at epoch=4=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5955 phrases; correct: 5886.\n",
      "accuracy:  99.43%; (non-O)\n",
      "accuracy:  99.86%; precision:  98.84%; recall:  99.06%; FB1:  98.95\n",
      "              LOC: precision:  99.67%; recall:  99.73%; FB1:  99.70  1838\n",
      "             MISC: precision:  97.29%; recall:  97.29%; FB1:  97.29  922\n",
      "              ORG: precision:  97.86%; recall:  98.73%; FB1:  98.29  1353\n",
      "              PER: precision:  99.51%; recall:  99.51%; FB1:  99.51  1842\n",
      "num_proposed:8600\n",
      "num_correct:8554\n",
      "num_gold:8603\n",
      "precision=0.9947\n",
      "recall=0.9943\n",
      "f1=0.9945\n",
      "weights were saved to checkpoints/02/4.pt\n",
      "=========test at epoch=4=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5701 phrases; correct: 5137.\n",
      "accuracy:  92.79%; (non-O)\n",
      "accuracy:  98.38%; precision:  90.11%; recall:  90.95%; FB1:  90.53\n",
      "              LOC: precision:  92.00%; recall:  92.33%; FB1:  92.16  1674\n",
      "             MISC: precision:  79.69%; recall:  79.91%; FB1:  79.80  704\n",
      "              ORG: precision:  88.05%; recall:  88.74%; FB1:  88.40  1674\n",
      "              PER: precision:  94.72%; recall:  96.60%; FB1:  95.65  1649\n",
      "num_proposed:8165\n",
      "num_correct:7527\n",
      "num_gold:8112\n",
      "precision=0.9219\n",
      "recall=0.9279\n",
      "f1=0.9249\n",
      "=========eval at epoch=5=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5957 phrases; correct: 5893.\n",
      "accuracy:  99.50%; (non-O)\n",
      "accuracy:  99.87%; precision:  98.93%; recall:  99.18%; FB1:  99.05\n",
      "              LOC: precision:  99.73%; recall:  99.73%; FB1:  99.73  1837\n",
      "             MISC: precision:  97.51%; recall:  97.72%; FB1:  97.62  924\n",
      "              ORG: precision:  97.93%; recall:  98.81%; FB1:  98.37  1353\n",
      "              PER: precision:  99.57%; recall:  99.62%; FB1:  99.59  1843\n",
      "num_proposed:8603\n",
      "num_correct:8560\n",
      "num_gold:8603\n",
      "precision=0.9950\n",
      "recall=0.9950\n",
      "f1=0.9950\n",
      "weights were saved to checkpoints/02/5.pt\n",
      "=========test at epoch=5=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5701 phrases; correct: 5135.\n",
      "accuracy:  92.80%; (non-O)\n",
      "accuracy:  98.38%; precision:  90.07%; recall:  90.92%; FB1:  90.49\n",
      "              LOC: precision:  92.16%; recall:  92.33%; FB1:  92.24  1671\n",
      "             MISC: precision:  79.40%; recall:  79.63%; FB1:  79.52  704\n",
      "              ORG: precision:  88.01%; recall:  88.86%; FB1:  88.44  1677\n",
      "              PER: precision:  94.60%; recall:  96.47%; FB1:  95.53  1649\n",
      "num_proposed:8167\n",
      "num_correct:7528\n",
      "num_gold:8112\n",
      "precision=0.9218\n",
      "recall=0.9280\n",
      "f1=0.9249\n",
      "=========eval at epoch=6=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5956 phrases; correct: 5900.\n",
      "accuracy:  99.58%; (non-O)\n",
      "accuracy:  99.89%; precision:  99.06%; recall:  99.29%; FB1:  99.18\n",
      "              LOC: precision:  99.78%; recall:  99.73%; FB1:  99.75  1836\n",
      "             MISC: precision:  98.16%; recall:  98.26%; FB1:  98.21  923\n",
      "              ORG: precision:  97.93%; recall:  98.96%; FB1:  98.44  1355\n",
      "              PER: precision:  99.62%; recall:  99.62%; FB1:  99.62  1842\n",
      "num_proposed:8604\n",
      "num_correct:8567\n",
      "num_gold:8603\n",
      "precision=0.9957\n",
      "recall=0.9958\n",
      "f1=0.9958\n",
      "weights were saved to checkpoints/02/6.pt\n",
      "=========test at epoch=6=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5706 phrases; correct: 5138.\n",
      "accuracy:  92.86%; (non-O)\n",
      "accuracy:  98.39%; precision:  90.05%; recall:  90.97%; FB1:  90.51\n",
      "              LOC: precision:  92.11%; recall:  92.33%; FB1:  92.22  1672\n",
      "             MISC: precision:  79.10%; recall:  79.77%; FB1:  79.43  708\n",
      "              ORG: precision:  88.13%; recall:  88.98%; FB1:  88.56  1677\n",
      "              PER: precision:  94.60%; recall:  96.47%; FB1:  95.53  1649\n",
      "num_proposed:8172\n",
      "num_correct:7533\n",
      "num_gold:8112\n",
      "precision=0.9218\n",
      "recall=0.9286\n",
      "f1=0.9252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========eval at epoch=7=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5954 phrases; correct: 5903.\n",
      "accuracy:  99.62%; (non-O)\n",
      "accuracy:  99.89%; precision:  99.14%; recall:  99.34%; FB1:  99.24\n",
      "              LOC: precision:  99.78%; recall:  99.78%; FB1:  99.78  1837\n",
      "             MISC: precision:  98.37%; recall:  98.37%; FB1:  98.37  922\n",
      "              ORG: precision:  98.01%; recall:  98.96%; FB1:  98.48  1354\n",
      "              PER: precision:  99.73%; recall:  99.67%; FB1:  99.70  1841\n",
      "num_proposed:8606\n",
      "num_correct:8570\n",
      "num_gold:8603\n",
      "precision=0.9958\n",
      "recall=0.9962\n",
      "f1=0.9960\n",
      "weights were saved to checkpoints/02/7.pt\n",
      "=========test at epoch=7=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5711 phrases; correct: 5135.\n",
      "accuracy:  92.79%; (non-O)\n",
      "accuracy:  98.36%; precision:  89.91%; recall:  90.92%; FB1:  90.41\n",
      "              LOC: precision:  92.05%; recall:  92.33%; FB1:  92.19  1673\n",
      "             MISC: precision:  78.95%; recall:  79.63%; FB1:  79.29  708\n",
      "              ORG: precision:  87.91%; recall:  88.86%; FB1:  88.38  1679\n",
      "              PER: precision:  94.49%; recall:  96.47%; FB1:  95.47  1651\n",
      "num_proposed:8173\n",
      "num_correct:7527\n",
      "num_gold:8112\n",
      "precision=0.9210\n",
      "recall=0.9279\n",
      "f1=0.9244\n",
      "=========eval at epoch=8=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5955 phrases; correct: 5905.\n",
      "accuracy:  99.65%; (non-O)\n",
      "accuracy:  99.90%; precision:  99.16%; recall:  99.38%; FB1:  99.27\n",
      "              LOC: precision:  99.78%; recall:  99.78%; FB1:  99.78  1837\n",
      "             MISC: precision:  98.48%; recall:  98.59%; FB1:  98.54  923\n",
      "              ORG: precision:  98.01%; recall:  98.96%; FB1:  98.48  1354\n",
      "              PER: precision:  99.73%; recall:  99.67%; FB1:  99.70  1841\n",
      "num_proposed:8608\n",
      "num_correct:8573\n",
      "num_gold:8603\n",
      "precision=0.9959\n",
      "recall=0.9965\n",
      "f1=0.9962\n",
      "weights were saved to checkpoints/02/8.pt\n",
      "=========test at epoch=8=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5709 phrases; correct: 5134.\n",
      "accuracy:  92.74%; (non-O)\n",
      "accuracy:  98.36%; precision:  89.93%; recall:  90.90%; FB1:  90.41\n",
      "              LOC: precision:  92.32%; recall:  92.21%; FB1:  92.26  1666\n",
      "             MISC: precision:  78.73%; recall:  79.63%; FB1:  79.18  710\n",
      "              ORG: precision:  87.86%; recall:  88.92%; FB1:  88.39  1681\n",
      "              PER: precision:  94.43%; recall:  96.47%; FB1:  95.44  1652\n",
      "num_proposed:8169\n",
      "num_correct:7523\n",
      "num_gold:8112\n",
      "precision=0.9209\n",
      "recall=0.9274\n",
      "f1=0.9241\n",
      "=========eval at epoch=9=========\n",
      "processed 55044 tokens with 5942 phrases; found: 5951 phrases; correct: 5906.\n",
      "accuracy:  99.66%; (non-O)\n",
      "accuracy:  99.91%; precision:  99.24%; recall:  99.39%; FB1:  99.32\n",
      "              LOC: precision:  99.78%; recall:  99.84%; FB1:  99.81  1838\n",
      "             MISC: precision:  98.70%; recall:  98.59%; FB1:  98.64  921\n",
      "              ORG: precision:  98.15%; recall:  98.96%; FB1:  98.55  1352\n",
      "              PER: precision:  99.78%; recall:  99.67%; FB1:  99.73  1840\n",
      "num_proposed:8604\n",
      "num_correct:8574\n",
      "num_gold:8603\n",
      "precision=0.9965\n",
      "recall=0.9966\n",
      "f1=0.9966\n",
      "weights were saved to checkpoints/02/9.pt\n",
      "=========test at epoch=9=========\n",
      "processed 50350 tokens with 5648 phrases; found: 5715 phrases; correct: 5134.\n",
      "accuracy:  92.71%; (non-O)\n",
      "accuracy:  98.36%; precision:  89.83%; recall:  90.90%; FB1:  90.36\n",
      "              LOC: precision:  92.25%; recall:  92.09%; FB1:  92.17  1665\n",
      "             MISC: precision:  78.35%; recall:  79.91%; FB1:  79.13  716\n",
      "              ORG: precision:  87.71%; recall:  88.92%; FB1:  88.31  1684\n",
      "              PER: precision:  94.55%; recall:  96.47%; FB1:  95.50  1650\n",
      "num_proposed:8170\n",
      "num_correct:7521\n",
      "num_gold:8112\n",
      "precision=0.9206\n",
      "recall=0.9271\n",
      "f1=0.9238\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    ensemble_model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        words, x, is_heads, _, y, seqlens = batch\n",
    "        \n",
    "        max_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        idx = i * batch_size\n",
    "        \n",
    "        if idx+batch_size > gcn_train.shape[0]:\n",
    "            break\n",
    "        gcn_tensor = gcn_train[idx:idx+batch_size]\n",
    "        padded = np.zeros((batch_size, max_len, 256))\n",
    "\n",
    "        for ix in range(batch_size):\n",
    "            is_heads[ix][0] = is_heads[ix][-1] = 0\n",
    "            num_word = sum(is_heads[ix])\n",
    "            indexs = [head == 1 for head in is_heads[ix]] + [False] * (max_len - len(is_heads[ix]))\n",
    "            padded[ix][indexs]=gcn_tensor[ix][:num_word]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = ensemble_model(x, padded)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        y = y.to(device).view(-1)\n",
    "        \n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "#         if i % 10 == 0:  # monitoring\n",
    "#             print(f\"step: {i}, loss: {loss.item()}\")\n",
    "        \n",
    "    print(f\"=========eval at epoch={ep}=========\")\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    fname = os.path.join(logdir, str(ep))\n",
    "    precision, recall, f1 = eval(ensemble_model, eval_iter, gcn_val, fname)\n",
    "\n",
    "    torch.save(model, f\"{fname}.pt\")\n",
    "    print(f\"weights were saved to {fname}.pt\")\n",
    "    print(f\"=========test at epoch={ep}=========\")\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    fname = os.path.join(logdir, str(ep)+'_test_')\n",
    "    precision, recall, f1 = eval(ensemble_model, test_iter, gcn_test, fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"=========eval=========\")\n",
    "# if not os.path.exists(logdir):\n",
    "#     os.makedirs(logdir)\n",
    "# fname = os.path.join(logdir)\n",
    "# precision, recall, f1 = eval(ensemble_model, eval_iter, gcn_val, fname)\n",
    "\n",
    "# torch.save(ensemble_model, f\"{fname}.pt\")\n",
    "# print(f\"weights were saved to {fname}.pt\")\n",
    "# print(f\"=========test=========\")\n",
    "# if not os.path.exists(logdir):\n",
    "#     os.makedirs(logdir)\n",
    "# fname = os.path.join(logdir,'_test_')\n",
    "# precision, recall, f1 = eval(ensemble_model, test_iter, gcn_test, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
